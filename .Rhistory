# creo la carpeta donde va el experimento
# HT  representa  Hiperparameter Tuning
dir.create("./exp/", showWarnings = FALSE)
dir.create("./exp/HT2020/", showWarnings = FALSE)
archivo_salida <- "./exp/HT2020/gridsearch.txt"
# genero la data.table donde van los resultados del Grid Search
tb_grid_search <- data.table( max_depth = integer(),
min_split = integer(),
ganancia_promedio = numeric() )
cp_values <- c(-1)
maxdepth_values <- c(14)
minsplit_values <- c(800)
minbucket_values <- c(240)
# Generate all combinations of parameters
param_grid <- expand.grid(
cp = cp_values,
minbucket = minbucket_values,
maxdepth = maxdepth_values,
minsplit = minsplit_values
)
# Define a function to run the model and get the average gain
run_model <- function(params) {
param_basicos <- list(
"cp" = params$cp,
"minsplit" = params$minsplit,
"minbucket" = params$minbucket,
"maxdepth" = params$maxdepth
)
ganancia_promedio <- ArbolesMontecarlo(PARAM$semillas, param_basicos)
print(paste("CP: " , as.character(as.numeric(params$cp)) , " MINSPLIT:" , as.character(params$minsplit) , " MINBUCKET: " , as.character(params$minbucket) , " MAXDEPTH:" , as.character(params$maxdepth)))
print(ganancia_promedio)
c(params, ganancia_promedio = ganancia_promedio)
}
# Apply the function to each combination of parameters
results <- do.call(rbind, lapply(1:nrow(param_grid), function(i) run_model(param_grid[i, ])))
ArbolesMontecarlo <- function(semillas, param_basicos) {
# la funcion mcmapply  llama a la funcion ArbolEstimarGanancia
#  tantas veces como valores tenga el vector  PARAM$semillas
ganancias <- mcmapply(ArbolEstimarGanancia,
semillas, # paso el vector de semillas
MoreArgs = list(param_basicos), # aqui paso el segundo parametro
SIMPLIFY = FALSE,
mc.cores = 1 # en Windows este valor debe ser 1
)
ganancia_promedio <- mean(unlist(ganancias))
return(ganancia_promedio)
}
# Generate all combinations of parameters
param_grid <- expand.grid(
cp = cp_values,
minbucket = minbucket_values,
maxdepth = maxdepth_values,
minsplit = minsplit_values
)
# Define a function to run the model and get the average gain
run_model <- function(params) {
param_basicos <- list(
"cp" = params$cp,
"minsplit" = params$minsplit,
"minbucket" = params$minbucket,
"maxdepth" = params$maxdepth
)
ganancia_promedio <- ArbolesMontecarlo(PARAM$semillas, param_basicos)
print(paste("CP: " , as.character(as.numeric(params$cp)) , " MINSPLIT:" , as.character(params$minsplit) , " MINBUCKET: " , as.character(params$minbucket) , " MAXDEPTH:" , as.character(params$maxdepth)))
print(ganancia_promedio)
c(params, ganancia_promedio = ganancia_promedio)
}
# Apply the function to each combination of parameters
results <- do.call(rbind, lapply(1:nrow(param_grid), function(i) run_model(param_grid[i, ])))
View(param_grid)
# Apply the function to each combination of parameters
results <- do.call(rbind, lapply(1:nrow(param_grid), function(i) run_model(param_grid[i, ])))
rm(list = ls()) # Borro todos los objetos
gc() # Garbage Collection
require("data.table")
require("rpart")
require("parallel")
PARAM <- list()
# reemplazar por las propias semillas
PARAM$semillas <- c(101599, 388177, 254747, 379289, 154027)
particionar <- function(data, division, agrupa = "", campo = "fold", start = 1, seed = NA) {
if (!is.na(seed)) set.seed(seed)
bloque <- unlist(mapply(function(x, y) {
rep(y, x)
}, division, seq(from = start, length.out = length(division))))
data[, (campo) := sample(rep(bloque, ceiling(.N / length(bloque))))[1:.N],
by = agrupa
]
}
ArbolEstimarGanancia <- function(semilla, param_basicos) {
# particiono estratificadamente el dataset
particionar(dataset, division = c(7, 3), agrupa = "clase_ternaria", seed = semilla)
# genero el modelo
# quiero predecir clase_ternaria a partir del resto
modelo <- rpart("clase_ternaria ~ .",
data = dataset[fold == 1], # fold==1  es training,  el 70% de los datos
xval = 0,
control = param_basicos
) # aqui van los parametros del arbol
# aplico el modelo a los datos de testing
prediccion <- predict(modelo, # el modelo que genere recien
dataset[fold == 2], # fold==2  es testing, el 30% de los datos
type = "prob"
) # type= "prob"  es que devuelva la probabilidad
# prediccion es una matriz con TRES columnas,
#  llamadas "BAJA+1", "BAJA+2"  y "CONTINUA"
# cada columna es el vector de probabilidades
# calculo la ganancia en testing  qu es fold==2
ganancia_test <- dataset[
fold == 2,
sum(ifelse(prediccion[, "BAJA+2"] > 0.025,
ifelse(clase_ternaria == "BAJA+2", 117000, -3000),
0
))
]
# escalo la ganancia como si fuera todo el dataset
ganancia_test_normalizada <- ganancia_test / 0.3
return(ganancia_test_normalizada)
}
ArbolesMontecarlo <- function(semillas, param_basicos) {
# la funcion mcmapply  llama a la funcion ArbolEstimarGanancia
#  tantas veces como valores tenga el vector  PARAM$semillas
ganancias <- mcmapply(ArbolEstimarGanancia,
semillas, # paso el vector de semillas
MoreArgs = list(param_basicos), # aqui paso el segundo parametro
SIMPLIFY = FALSE,
mc.cores = 1 # en Windows este valor debe ser 1
)
ganancia_promedio <- mean(unlist(ganancias))
return(ganancia_promedio)
}
# Aqui se debe poner la carpeta de la computadora local
setwd("C:\\Users\\marcelo.sambrizzi\\OneDrive - LUNDIN MINING CORPORATION\\Documents\\Maestria\\Laboratorio_Implementación_I\\labo2024v1") # Establezco el Working Directory
# cargo los datos
dataset <- fread("../datasets/dataset_pequeno.csv")
# trabajo solo con los datos con clase, es decir 202107
dataset <- dataset[clase_ternaria != ""]
# genero el archivo para Kaggle
# creo la carpeta donde va el experimento
# HT  representa  Hiperparameter Tuning
dir.create("./exp/", showWarnings = FALSE)
dir.create("./exp/HT2020/", showWarnings = FALSE)
archivo_salida <- "./exp/HT2020/gridsearch_Eli.txt"
# genero la data.table donde van los resultados del Grid Search
tb_grid_search <- data.table( minbucket = integer(),
cp = numeric(),
max_depth = integer(),
min_split = integer(),
ganancia_promedio = numeric() )
# itero por los loops anidados para cada hiperparametro
for (vminbucket in c(240)) {
for (vcp in c(-1)) {
for (vmax_depth in c(14)) {
for (vmin_split in c(800)) {
# notar como se agrega
# vminsplit  minima cantidad de registros en un nodo para hacer el split
param_basicos <- list(
"cp" = vcp, # complejidad minima
"minsplit" = vmin_split,
"minbucket" = vminbucket, # minima cantidad de registros en una hoja
"maxdepth" = vmax_depth
) # profundidad máxima del arbol
# Un solo llamado, con la semilla 17
ganancia_promedio <- ArbolesMontecarlo(PARAM$semillas, param_basicos)
# agrego a la tabla
tb_grid_search <- rbindlist(
list( tb_grid_search,
list( vminbucket, vcp, vmax_depth, vmin_split, ganancia_promedio) ) )
}
}
}
# escribo la tabla a disco en cada vuelta del loop mas externo
Sys.sleep(2)  # espero un par de segundos
fwrite( tb_grid_search,
file = archivo_salida,
sep = "\t" )
}
# cargo las librerias que necesito
require("data.table")
require("rpart")
require("rpart.plot")
# Aqui se debe poner la carpeta de la materia de SU computadora local
setwd("C:\\Users\\marcelo.sambrizzi\\OneDrive - LUNDIN MINING CORPORATION\\Documents\\Maestria\\Laboratorio_Implementación_I\\labo2024v1") # Establezco el Working Directory
# cargo el dataset
dataset <- fread("../datasets/dataset_pequeno.csv")
dtrain <- dataset[foto_mes == 202107] # defino donde voy a entrenar
dapply <- dataset[foto_mes == 202109] # defino donde voy a aplicar el modelo
# genero el modelo,  aqui se construye el arbol
# quiero predecir clase_ternaria a partir de el resto de las variables
modelo <- rpart(
formula = "clase_ternaria ~ .",
data = dtrain, # los datos donde voy a entrenar
xval = 0,
cp = -0.92466724144037, # esto significa no limitar la complejidad de los splits
minsplit = 1500, # minima cantidad de registros para que se haga el split
minbucket = 766, # tamaño minimo de una hoja
maxdepth = 20
) # profundidad maxima del arbol
# grafico el arbol
prp(modelo,
extra = 101, digits = -5,
branch = 1, type = 4, varlen = 0, faclen = 0
)
# aplico el modelo a los datos nuevos
prediccion <- predict(
object = modelo,
newdata = dapply,
type = "prob"
)
# agrego a dapply una columna nueva que es la probabilidad de BAJA+2
dapply[, prob_baja2 := prediccion[, "BAJA+2"]]
# solo le envio estimulo a los registros
#  con probabilidad de BAJA+2 mayor  a  1/40
dapply[, Predicted := as.numeric(prob_baja2 > 1/40)]
# genero el archivo para Kaggle
# primero creo la carpeta donde va el experimento
dir.create("./exp/")
dir.create("./exp/KA2001")
# solo los campos para Kaggle
fwrite(dapply[, list(numero_de_cliente, Predicted)],
file = "./exp/KA2001/K101_001.csv",
sep = ","
)
# cargo las librerias que necesito
require("data.table")
require("rpart")
require("rpart.plot")
# Aqui se debe poner la carpeta de la materia de SU computadora local
setwd("C:\\Users\\marcelo.sambrizzi\\OneDrive - LUNDIN MINING CORPORATION\\Documents\\Maestria\\Laboratorio_Implementación_I\\labo2024v1") # Establezco el Working Directory
# cargo el dataset
dataset <- fread("../datasets/dataset_pequeno.csv")
dtrain <- dataset[foto_mes == 202107] # defino donde voy a entrenar
dapply <- dataset[foto_mes == 202109] # defino donde voy a aplicar el modelo
# genero el modelo,  aqui se construye el arbol
# quiero predecir clase_ternaria a partir de el resto de las variables
modelo <- rpart(
formula = "clase_ternaria ~ .",
data = dtrain, # los datos donde voy a entrenar
xval = 0,
cp = -0.58715616154358, # esto significa no limitar la complejidad de los splits
minsplit = 1193, # minima cantidad de registros para que se haga el split
minbucket = 590, # tamaño minimo de una hoja
maxdepth = 12
) # profundidad maxima del arbol
# grafico el arbol
prp(modelo,
extra = 101, digits = -5,
branch = 1, type = 4, varlen = 0, faclen = 0
)
# aplico el modelo a los datos nuevos
prediccion <- predict(
object = modelo,
newdata = dapply,
type = "prob"
)
# agrego a dapply una columna nueva que es la probabilidad de BAJA+2
dapply[, prob_baja2 := prediccion[, "BAJA+2"]]
# solo le envio estimulo a los registros
#  con probabilidad de BAJA+2 mayor  a  1/40
dapply[, Predicted := as.numeric(prob_baja2 > 1/40)]
# genero el archivo para Kaggle
# primero creo la carpeta donde va el experimento
dir.create("./exp/")
dir.create("./exp/KA2001")
# solo los campos para Kaggle
fwrite(dapply[, list(numero_de_cliente, Predicted)],
file = "./exp/KA2001/K101_001.csv",
sep = ","
)
# cargo las librerias que necesito
require("data.table")
require("rpart")
require("rpart.plot")
# Aqui se debe poner la carpeta de la materia de SU computadora local
setwd("C:\\Users\\marcelo.sambrizzi\\OneDrive - LUNDIN MINING CORPORATION\\Documents\\Maestria\\Laboratorio_Implementación_I\\labo2024v1") # Establezco el Working Directory
# cargo el dataset
dataset <- fread("../datasets/dataset_pequeno.csv")
dtrain <- dataset[foto_mes == 202107] # defino donde voy a entrenar
dapply <- dataset[foto_mes == 202109] # defino donde voy a aplicar el modelo
# genero el modelo,  aqui se construye el arbol
# quiero predecir clase_ternaria a partir de el resto de las variables
modelo <- rpart(
formula = "clase_ternaria ~ .",
data = dtrain, # los datos donde voy a entrenar
xval = 0,
cp = -0.9089776405, # esto significa no limitar la complejidad de los splits
minsplit = 1430, # minima cantidad de registros para que se haga el split
minbucket = 712, # tamaño minimo de una hoja
maxdepth = 13
) # profundidad maxima del arbol
# grafico el arbol
prp(modelo,
extra = 101, digits = -5,
branch = 1, type = 4, varlen = 0, faclen = 0
)
# aplico el modelo a los datos nuevos
prediccion <- predict(
object = modelo,
newdata = dapply,
type = "prob"
)
# agrego a dapply una columna nueva que es la probabilidad de BAJA+2
dapply[, prob_baja2 := prediccion[, "BAJA+2"]]
# solo le envio estimulo a los registros
#  con probabilidad de BAJA+2 mayor  a  1/40
dapply[, Predicted := as.numeric(prob_baja2 > 1/40)]
# genero el archivo para Kaggle
# primero creo la carpeta donde va el experimento
dir.create("./exp/")
dir.create("./exp/KA2001")
# solo los campos para Kaggle
fwrite(dapply[, list(numero_de_cliente, Predicted)],
file = "./exp/KA2001/K101_001.csv",
sep = ","
)
# cargo las librerias que necesito
require("data.table")
require("rpart")
require("rpart.plot")
# Aqui se debe poner la carpeta de la materia de SU computadora local
setwd("C:\\Users\\marcelo.sambrizzi\\OneDrive - LUNDIN MINING CORPORATION\\Documents\\Maestria\\Laboratorio_Implementación_I\\labo2024v1") # Establezco el Working Directory
# cargo el dataset
dataset <- fread("../datasets/dataset_pequeno.csv")
dtrain <- dataset[foto_mes == 202107] # defino donde voy a entrenar
dapply <- dataset[foto_mes == 202109] # defino donde voy a aplicar el modelo
# genero el modelo,  aqui se construye el arbol
# quiero predecir clase_ternaria a partir de el resto de las variables
modelo <- rpart(
formula = "clase_ternaria ~ .",
data = dtrain, # los datos donde voy a entrenar
xval = 0,
cp = -0.2311984823, # esto significa no limitar la complejidad de los splits
minsplit = 8000, # minima cantidad de registros para que se haga el split
minbucket = 2740, # tamaño minimo de una hoja
maxdepth = 20
) # profundidad maxima del arbol
# grafico el arbol
prp(modelo,
extra = 101, digits = -5,
branch = 1, type = 4, varlen = 0, faclen = 0
)
# aplico el modelo a los datos nuevos
prediccion <- predict(
object = modelo,
newdata = dapply,
type = "prob"
)
# agrego a dapply una columna nueva que es la probabilidad de BAJA+2
dapply[, prob_baja2 := prediccion[, "BAJA+2"]]
# solo le envio estimulo a los registros
#  con probabilidad de BAJA+2 mayor  a  1/40
dapply[, Predicted := as.numeric(prob_baja2 > 1/40)]
# genero el archivo para Kaggle
# primero creo la carpeta donde va el experimento
dir.create("./exp/")
dir.create("./exp/KA2001")
# solo los campos para Kaggle
fwrite(dapply[, list(numero_de_cliente, Predicted)],
file = "./exp/KA2001/K101_001.csv",
sep = ","
)
# cargo las librerias que necesito
require("data.table")
require("rpart")
require("rpart.plot")
# Aqui se debe poner la carpeta de la materia de SU computadora local
setwd("C:\\Users\\marcelo.sambrizzi\\OneDrive - LUNDIN MINING CORPORATION\\Documents\\Maestria\\Laboratorio_Implementación_I\\labo2024v1") # Establezco el Working Directory
# cargo el dataset
dataset <- fread("../datasets/dataset_pequeno.csv")
dtrain <- dataset[foto_mes == 202107] # defino donde voy a entrenar
dapply <- dataset[foto_mes == 202109] # defino donde voy a aplicar el modelo
# genero el modelo,  aqui se construye el arbol
# quiero predecir clase_ternaria a partir de el resto de las variables
modelo <- rpart(
formula = "clase_ternaria ~ .",
data = dtrain, # los datos donde voy a entrenar
xval = 0,
cp = -0.8356289342, # esto significa no limitar la complejidad de los splits
minsplit = 7579, # minima cantidad de registros para que se haga el split
minbucket = 2504, # tamaño minimo de una hoja
maxdepth = 20
) # profundidad maxima del arbol
# grafico el arbol
prp(modelo,
extra = 101, digits = -5,
branch = 1, type = 4, varlen = 0, faclen = 0
)
# aplico el modelo a los datos nuevos
prediccion <- predict(
object = modelo,
newdata = dapply,
type = "prob"
)
# agrego a dapply una columna nueva que es la probabilidad de BAJA+2
dapply[, prob_baja2 := prediccion[, "BAJA+2"]]
# solo le envio estimulo a los registros
#  con probabilidad de BAJA+2 mayor  a  1/40
dapply[, Predicted := as.numeric(prob_baja2 > 1/40)]
# genero el archivo para Kaggle
# primero creo la carpeta donde va el experimento
dir.create("./exp/")
dir.create("./exp/KA2001")
# solo los campos para Kaggle
fwrite(dapply[, list(numero_de_cliente, Predicted)],
file = "./exp/KA2001/K101_001.csv",
sep = ","
)
rm(list = ls()) # Borro todos los objetos
gc() # Garbage Collection
require("data.table")
require("rpart")
require("parallel")
PARAM <- list()
# reemplazar por las propias semillas
PARAM$semillas <- c(101599, 388177, 254747, 379289, 154027)
particionar <- function(data, division, agrupa = "", campo = "fold", start = 1, seed = NA) {
if (!is.na(seed)) set.seed(seed)
bloque <- unlist(mapply(function(x, y) {
rep(y, x)
}, division, seq(from = start, length.out = length(division))))
data[, (campo) := sample(rep(bloque, ceiling(.N / length(bloque))))[1:.N],
by = agrupa
]
}
ArbolEstimarGanancia <- function(semilla, param_basicos) {
# particiono estratificadamente el dataset
particionar(dataset, division = c(7, 3), agrupa = "clase_ternaria", seed = semilla)
# genero el modelo
# quiero predecir clase_ternaria a partir del resto
modelo <- rpart("clase_ternaria ~ .",
data = dataset[fold == 1], # fold==1  es training,  el 70% de los datos
xval = 0,
control = param_basicos
) # aqui van los parametros del arbol
# aplico el modelo a los datos de testing
prediccion <- predict(modelo, # el modelo que genere recien
dataset[fold == 2], # fold==2  es testing, el 30% de los datos
type = "prob"
) # type= "prob"  es que devuelva la probabilidad
# prediccion es una matriz con TRES columnas,
#  llamadas "BAJA+1", "BAJA+2"  y "CONTINUA"
# cada columna es el vector de probabilidades
# calculo la ganancia en testing  qu es fold==2
ganancia_test <- dataset[
fold == 2,
sum(ifelse(prediccion[, "BAJA+2"] > 0.025,
ifelse(clase_ternaria == "BAJA+2", 117000, -3000),
0
))
]
# escalo la ganancia como si fuera todo el dataset
ganancia_test_normalizada <- ganancia_test / 0.3
return(ganancia_test_normalizada)
}
ArbolesMontecarlo <- function(semillas, param_basicos) {
# la funcion mcmapply  llama a la funcion ArbolEstimarGanancia
#  tantas veces como valores tenga el vector  PARAM$semillas
ganancias <- mcmapply(ArbolEstimarGanancia,
semillas, # paso el vector de semillas
MoreArgs = list(param_basicos), # aqui paso el segundo parametro
SIMPLIFY = FALSE,
mc.cores = 1 # en Windows este valor debe ser 1
)
ganancia_promedio <- mean(unlist(ganancias))
return(ganancia_promedio)
}
# Aqui se debe poner la carpeta de la computadora local
setwd("C:\\Users\\marcelo.sambrizzi\\OneDrive - LUNDIN MINING CORPORATION\\Documents\\Maestria\\Laboratorio_Implementación_I\\labo2024v1") # Establezco el Working Directory
# cargo los datos
dataset <- fread("../datasets/dataset_pequeno.csv")
# trabajo solo con los datos con clase, es decir 202107
dataset <- dataset[clase_ternaria != ""]
# genero el archivo para Kaggle
# creo la carpeta donde va el experimento
# HT  representa  Hiperparameter Tuning
dir.create("./exp/", showWarnings = FALSE)
dir.create("./exp/HT2020/", showWarnings = FALSE)
archivo_salida <- "./exp/HT2020/gridsearch_Eli.txt"
# genero la data.table donde van los resultados del Grid Search
tb_grid_search <- data.table( minbucket = integer(),
cp = numeric(),
max_depth = integer(),
min_split = integer(),
ganancia_promedio = numeric() )
# itero por los loops anidados para cada hiperparametro
for (vminbucket in c(998)) {
for (vcp in c(-0.990134449765289)) {
for (vmax_depth in c(17)) {
for (vmin_split in c(2002)) {
# notar como se agrega
# vminsplit  minima cantidad de registros en un nodo para hacer el split
param_basicos <- list(
"cp" = vcp, # complejidad minima
"minsplit" = vmin_split,
"minbucket" = vminbucket, # minima cantidad de registros en una hoja
"maxdepth" = vmax_depth
) # profundidad máxima del arbol
# Un solo llamado, con la semilla 17
ganancia_promedio <- ArbolesMontecarlo(PARAM$semillas, param_basicos)
# agrego a la tabla
tb_grid_search <- rbindlist(
list( tb_grid_search,
list( vminbucket, vcp, vmax_depth, vmin_split, ganancia_promedio) ) )
}
}
}
# escribo la tabla a disco en cada vuelta del loop mas externo
Sys.sleep(2)  # espero un par de segundos
fwrite( tb_grid_search,
file = archivo_salida,
sep = "\t" )
}
